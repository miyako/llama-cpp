# llama.cpp
Local inference engine
